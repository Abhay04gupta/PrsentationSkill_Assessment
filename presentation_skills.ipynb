{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers textstat nltk textblob vaderSentiment langdetect transformers language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rkMCHlkOsmQQ",
        "outputId": "8c22892e-5d69-44cd-fbfc-cc1c4a39f73e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (24.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.12.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
            "Downloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=2bea7be01a1934e184c0625fc7ba7549152e7a22cfaeb8efd718ba5744bd92b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pyphen, langdetect, vaderSentiment, textstat, language-tool-python\n",
            "Successfully installed langdetect-1.0.9 language-tool-python-2.8.1 pyphen-0.17.0 textstat-0.7.4 vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/PrithivirajDamodaran/Gramformer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us9jmq7CGiBu",
        "outputId": "6ba2f291-7973-4c47-8daa-7c9ee0f2f0f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
            "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to /tmp/pip-req-build-zqf4s66n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Gramformer.git /tmp/pip-req-build-zqf4s66n\n",
            "  Resolved https://github.com/PrithivirajDamodaran/Gramformer.git to commit 23425cd2e98a919384cab6156af8adf1c9d0639a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (4.47.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (0.2.0)\n",
            "Collecting python-Levenshtein (from gramformer==1.0)\n",
            "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting fuzzywuzzy (from gramformer==1.0)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (0.21.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (2024.10.0)\n",
            "Collecting errant (from gramformer==1.0)\n",
            "  Downloading errant-3.0.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from errant->gramformer==1.0) (3.7.5)\n",
            "Collecting rapidfuzz>=3.4.0 (from errant->gramformer==1.0)\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting Levenshtein==0.26.1 (from python-Levenshtein->gramformer==1.0)\n",
            "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->gramformer==1.0) (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->gramformer==1.0) (4.12.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (0.15.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (2024.12.14)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.27.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.1.2)\n",
            "Downloading errant-3.0.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.3/499.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gramformer\n",
            "  Building wheel for gramformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gramformer: filename=gramformer-1.0-py3-none-any.whl size=4461 sha256=1bb18eb02cee184848519e256d7cbae2f4ea51812bd7feb2645d129d0450c244\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pa4jhlbo/wheels/76/44/15/e79b5dc4f5c897b2054e6a8e357f6de157b9554f072a2e56ea\n",
            "Successfully built gramformer\n",
            "Installing collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein, errant, gramformer\n",
            "Successfully installed Levenshtein-0.26.1 errant-3.0.0 fuzzywuzzy-0.18.0 gramformer-1.0 python-Levenshtein-0.26.1 rapidfuzz-3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG8gDuk2sdSK",
        "outputId": "59b5602b-66f9-4744-b1ef-565a9b657b39",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import language_tool_python\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import textstat\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from langdetect import detect\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from gramformer import Gramformer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Presentation Assessment Pipeline\n",
        "# Initialize tools and models\n",
        "spell_checker=language_tool_python.LanguageTool('en-US') # Spell-Checker\n",
        "gf = Gramformer(models=1)  # Set models=1 for grammar correction\n",
        "relevancy_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # Relevancy Model\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')  # Semantic similarity\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()  # Sentiment analysis\n",
        "politeness_analyzer= SentimentIntensityAnalyzer()  # Politeness Analyzer\n",
        "coherency_analyzer= SentenceTransformer('all-MiniLM-L6-v2') # Coherency Analyzer\n",
        "\n",
        "\n",
        "def presentation_skill_ass(content,topic):\n",
        "    # Assessment Criteria\n",
        "    def evaluate_spell_with_textblob(content):\n",
        "        text = spell_checker.check(content)\n",
        "        corrected_content = language_tool_python.utils.correct(content,text)\n",
        "        lines=content.strip().split(\"\\n\")\n",
        "        incorrect=0\n",
        "        for line in lines:\n",
        "            if line !=\"\":\n",
        "              prev_words=line.split(\" \")\n",
        "              incorrect_line = spell_checker.check(line)\n",
        "              correct_line=language_tool_python.utils.correct(line,incorrect_line)\n",
        "              new_words=correct_line.split(\" \")\n",
        "              for prev_word,new_word in zip(prev_words,new_words):\n",
        "                if prev_word != new_word:\n",
        "                  incorrect+=1\n",
        "        return incorrect,corrected_content\n",
        "\n",
        "\n",
        "    def grammatical_error(content):\n",
        "        #content processing\n",
        "        sp_lines=content.strip().split(\"\\n\")\n",
        "        lines=[]\n",
        "\n",
        "        for line in sp_lines:\n",
        "            lis=line.split(\".\")\n",
        "            for l in lis:\n",
        "              if l!=\"\":\n",
        "                lines.append(l.strip())\n",
        "\n",
        "        incorrect=0\n",
        "        errors={}\n",
        "\n",
        "        for line in lines:\n",
        "          if line!=\"\":\n",
        "              line=re.sub(r\"[^\\w\\s\\']\", \"\", line)\n",
        "              prev_words=line.split(\" \")\n",
        "\n",
        "              # Correct grammar\n",
        "              corrected_line = gf.correct(line)\n",
        "              corrected_line_without_pun=next(iter(corrected_line))\n",
        "              corrected_line_without_pun=re.sub(r\"[^\\w\\s\\']\",\"\",corrected_line_without_pun)\n",
        "              new_words=corrected_line_without_pun.split()\n",
        "\n",
        "\n",
        "              for prev_word,new_word in zip(prev_words,new_words):\n",
        "                if prev_word != new_word:\n",
        "                  errors[line]=corrected_line_without_pun\n",
        "                  incorrect+=1\n",
        "                  break\n",
        "\n",
        "        return incorrect,round((10-((incorrect*10)/len(lines))),0) ,errors\n",
        "\n",
        "    def evaluate_relevance(content, topic):\n",
        "        topic_embedding = relevancy_model.encode(topic, convert_to_tensor=True)\n",
        "        speech_embedding = relevancy_model.encode(content, convert_to_tensor=True)\n",
        "\n",
        "        # Compute similarity\n",
        "        similarity_score = util.cos_sim(topic_embedding, speech_embedding).item()\n",
        "\n",
        "        if similarity_score < 0.45:\n",
        "            similarity_tag = \"Irrelevant\"\n",
        "        elif similarity_score < 0.5:\n",
        "            similarity_tag = \"Relevant\"\n",
        "        else:\n",
        "            similarity_tag = \"Highly Relevant\"\n",
        "\n",
        "        return round(similarity_score, 2), similarity_tag\n",
        "\n",
        "    def evaluate_sentiment(content):\n",
        "        sentences = sent_tokenize(content)\n",
        "        scores = [sentiment_analyzer.polarity_scores(sent)['compound'] for sent in sentences]\n",
        "        avg_sentiment = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        #tagging for sentiment\n",
        "        if avg_sentiment>0:\n",
        "            sentiment_tag=\"Positive\"\n",
        "        elif avg_sentiment<0:\n",
        "            sentiment_tag=\"Negative\"\n",
        "        else:\n",
        "            sentiment_tag=\"Neutral\"\n",
        "        return round(avg_sentiment,2),sentiment_tag\n",
        "\n",
        "    def evaluate_readability(content):\n",
        "        flesch_score = textstat.flesch_reading_ease(content)\n",
        "\n",
        "        #tagging for readability\n",
        "        if flesch_score>60:\n",
        "            flesch_tag=\"Easy to Read\"\n",
        "        elif 30<flesch_score<60:\n",
        "            flesch_tag=\"Moderate Readability\"\n",
        "        else:\n",
        "            flesch_tag=\"Difficult to Read\"\n",
        "\n",
        "        return round(flesch_score,2),flesch_tag\n",
        "\n",
        "    def detect_politeness(text):\n",
        "        politeness_score = 0\n",
        "\n",
        "        # Polite phrases\n",
        "        polite_phrases = [\"please\", \"kindly\", \"thank you\", \"would you\", \"could you\", \"I would appreciate\", \"sorry\", \"I apologize\"]\n",
        "        politeness_score += sum(phrase in text.lower() for phrase in polite_phrases)\n",
        "\n",
        "        # Modal verbs\n",
        "        modal_verbs = [\"would\", \"could\", \"may\", \"might\", \"should\"]\n",
        "        politeness_score += sum(modal in text.lower() for modal in modal_verbs)\n",
        "\n",
        "        # Indirect requests\n",
        "        indirect_phrases = [\"I was wondering\", \"is it possible\", \"could you please\", \"would it be okay if\"]\n",
        "        politeness_score += 2 * sum(phrase in text.lower() for phrase in indirect_phrases)\n",
        "\n",
        "        # Greetings and closings\n",
        "        greetings = [\"dear\", \"hello\", \"good morning\", \"good evening\"]\n",
        "        closings = [\"sincerely\", \"kind regards\", \"best regards\", \"yours faithfully\"]\n",
        "        politeness_score += sum(phrase in text.lower() for phrase in greetings + closings)\n",
        "\n",
        "        # Gratitude and apologies\n",
        "        gratitude_phrases = [\"thank you\", \"I appreciate\", \"thanks\", \"I am grateful\"]\n",
        "        apology_phrases = [\"I apologize\", \"sorry\", \"excuse me\"]\n",
        "        politeness_score += sum(phrase in text.lower() for phrase in gratitude_phrases + apology_phrases)\n",
        "\n",
        "        # Hedging language\n",
        "        hedging_phrases = [\"it seems\", \"I think\", \"perhaps\", \"it might be better to\"]\n",
        "        politeness_score += sum(phrase in text.lower() for phrase in hedging_phrases)\n",
        "\n",
        "        # Avoidance of commands\n",
        "        if text.lower().startswith(\"send\") or \"do this\" in text.lower():\n",
        "            politeness_score -= 1\n",
        "\n",
        "        # Sentence structure\n",
        "        if len(text.split(',')) > 1:\n",
        "            politeness_score += 1\n",
        "\n",
        "        # Sentiment analysis\n",
        "        from textblob import TextBlob\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        if sentiment > 0:\n",
        "            politeness_score += 1\n",
        "\n",
        "        # Final classification\n",
        "        if politeness_score > 6:\n",
        "            return politeness_score,\"Highly Polite\"\n",
        "        elif 3 < politeness_score <= 6:\n",
        "            return politeness_score,\"Moderately Polite\"\n",
        "        else:\n",
        "            return politeness_score,\"Impolite or Neutral\"\n",
        "\n",
        "    def coherence(content):\n",
        "        sentences=content.split(\".\")\n",
        "        # Compute embeddings\n",
        "        embeddings = coherency_analyzer.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "        # Compute pairwise cosine similarities\n",
        "        similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
        "\n",
        "        # Display coherence scores\n",
        "        score=0\n",
        "        for i in range(len(similarities)):\n",
        "            for j in range(len(similarities[0])):\n",
        "                if j!=i:\n",
        "                    score+=similarities[i][j]\n",
        "        n=len(sentences)\n",
        "        coherence_score=score/((n**2)-n)\n",
        "        coherence_tag = \"Excellent\" if coherence_score >=0.35 else \"Good\" if 0.20< coherence_score <0.35 else \"Poor\"\n",
        "\n",
        "        return round(float(coherence_score),2),coherence_tag\n",
        "\n",
        "    def email_structure(email):\n",
        "        structure_categories={\n",
        "            \"subject\":[\"Subject\"],\n",
        "\n",
        "            \"greeting\":[\n",
        "                      \"Dear\", \"Hi\", \"Hello\", \"To Whom It May Concern\", \"Respected\",\n",
        "                      \"Good Morning\", \"Good Afternoon\", \"Good Evening\", \"Greetings\",\n",
        "                      \"Dear Sir\", \"Dear Madam\",\"Dear Team\", \"Dear All\", \"Hi Everyone\", \"Hello Team\",\n",
        "                      \"Dear Colleagues\", \"Dear Members\"\n",
        "                      ],\n",
        "\n",
        "            \"closing\":[\n",
        "                      \"Best regards\", \"Sincerely\", \"Warm regards\", \"Thank you\", \"Thanking you\", \"Thanks\",\n",
        "                      \"Yours sincerely\", \"Yours faithfully\", \"Kind regards\", \"Warm wishes\", \"Best wishes\",\n",
        "                      \"Regards\", \"Thanks and regards\", \"Cheers\", \"Take care\", \"Respectfully\",\n",
        "                      \"Looking forward to your response\", \"With appreciation\", \"Cordially\",\n",
        "                      \"Yours truly\", \"Faithfully yours\"\n",
        "                     ]\n",
        "        }\n",
        "\n",
        "        lines = email.strip().split(\"\\n\")\n",
        "\n",
        "        results = {\n",
        "            \"has_subject\": False,\n",
        "            \"has_greeting\": False,\n",
        "            \"has_closing\": False,\n",
        "        }\n",
        "\n",
        "        score=0\n",
        "\n",
        "        #check for subject\n",
        "        for line in lines:\n",
        "            if line.startswith(\"Subject\"):\n",
        "                score=score+1 if results[\"has_subject\"] == False else score\n",
        "                results[\"has_subject\"] = True\n",
        "                break\n",
        "\n",
        "        #check for greeting\n",
        "        for line in lines:\n",
        "            for greeting in structure_categories[\"greeting\"]:\n",
        "                if greeting in line:\n",
        "                    score=score+1 if results[\"has_greeting\"] == False else score\n",
        "                    results[\"has_greeting\"] = True\n",
        "                    break\n",
        "\n",
        "        #check for closing\n",
        "        for line in lines:\n",
        "            for closing in structure_categories[\"closing\"]:\n",
        "                if closing in line:\n",
        "                    score=score+1 if results[\"has_closing\"] == False else score\n",
        "                    results[\"has_closing\"] = True\n",
        "                    break\n",
        "\n",
        "\n",
        "        return score,results\n",
        "\n",
        "\n",
        "    #score\n",
        "    score=0\n",
        "\n",
        "    # Perform Assessments\n",
        "    spell_inc_score, corrected_content = evaluate_spell_with_textblob(content)\n",
        "    grammatical_inc_score, grammatical_score_of_10, grammatical_errors = grammatical_error(content)\n",
        "    score+=grammatical_score_of_10\n",
        "    relevance_score, relevancy_tag = evaluate_relevance(str(corrected_content), topic)\n",
        "    score=score+10 if relevancy_tag==\"Highly Relevant\" else score+9 if relevancy_tag==\"Relevant\" else score+8\n",
        "    sentiment_score, sentiment_tag = evaluate_sentiment(str(corrected_content))\n",
        "    score=score+10 if sentiment_tag==\"Positive\" else score+9\n",
        "    readability_score, readability_tag = evaluate_readability(str(corrected_content))\n",
        "    score=score+10 if readability_tag==\"Easy to Read\" else score+9 if readability_tag==\"Moderate Readability\" else score+8\n",
        "    politeness_score, politeness_tag = detect_politeness(str(corrected_content))\n",
        "    score=score+10 if politeness_tag==\"Highly Polite\" else score+9 if politeness_tag==\"Moderately Polite\" else score+8\n",
        "    coherency_score, coherency_tag = coherence(str(corrected_content))\n",
        "    score=score+10 if coherency_tag==\"Excellent\" else score+9 if coherency_tag==\"Good\" else score+8\n",
        "    email_structure_score, email_structure_metric = email_structure(content)\n",
        "    score+=email_structure_score\n",
        "\n",
        "    #normalizing score\n",
        "    score=round((score*10)/63,2)\n",
        "\n",
        "\n",
        "    # Generate Feedback\n",
        "    feedback = {\n",
        "        \"Grammar incorections\": grammatical_inc_score,\n",
        "        \"Grammar_score\": f\"{grammatical_score_of_10}/10\",\n",
        "        \"Grammatical_errors\": grammatical_errors,\n",
        "        \"Relevancy_type\": relevancy_tag,\n",
        "        \"Sentiment_type\": sentiment_tag,\n",
        "        \"Readability_type\": readability_tag,\n",
        "        \"Politeness type\": politeness_tag,\n",
        "        \"Coherency_type\": coherency_tag,\n",
        "        \"Email_structure_score\": f\"{email_structure_score}/3\",\n",
        "        \"Email_structure_type\": email_structure_metric,\n",
        "        \"Overall Score\": f\"{score}/10\"\n",
        "    }\n",
        "\n",
        "    # Display Feedback\n",
        "    for key, value in feedback.items():\n",
        "        print(f\"{key}: {value}\")\n"
      ],
      "metadata": {
        "id": "MOTrzvwC1W1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0994daf9-dc5f-474c-da6f-ebda9ceb70b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gramformer] Grammar error correct/highlight model loaded..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Grammar Errors: A high count indicates issues with clarity and professionalism.\n",
        "#Relevance Score: A score close to 1.0 means the content is highly relevant to the topic.\n",
        "#Sentiment Score: Positive scores indicate engaging and optimistic tones, while negative scores suggest the opposite.\n",
        "#Readability Score: Higher scores (60-100) are easier to read for a general audience."
      ],
      "metadata": {
        "id": "ZH1k7_J0wJ6k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_content=\"\"\"\n",
        "Subject: Request for Extension on Project Deadline\n",
        "\n",
        "Dear Abhay,\n",
        "\n",
        "I hope this email finds you well. I am writing to request a possible extension for the submission of the deadline, which is currently set for.\n",
        "\n",
        "Due to unforeseen circumstances, our team has encountered delays in. We believe that an extension of would allow us to deliver work of higher quality that meets the expectations set for this project.\n",
        "\n",
        "We deeply appreciate your understanding and are willing to discuss this matter further if needed. Please let us know if a formal meeting is required to address this request.\n",
        "\n",
        "Thank you for considering our situation. We value your support and guidance.\n",
        "\n",
        "Best regards,\n",
        "Rakesh\n",
        "\n",
        "\"\"\"\n",
        "topic = \"Deadline Extension\"\n",
        "\n",
        "presentation_skill_ass(user_content,topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkSlLYL074i8",
        "outputId": "028f1f93-b453-4a59-c025-76e412d74e9d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar incorections: 2\n",
            "Grammar_score: 8.0/10\n",
            "Grammatical_errors: {'Subject Request for Extension on Project Deadline': 'Subject Request for Extension of Project Deadline', 'We believe that an extension of would allow us to deliver work of higher quality that meets the expectations set for this project': 'We believe that an extension would allow us to deliver work of higher quality that meets the expectations set for this project'}\n",
            "Relevancy_type: Highly Relevant\n",
            "Sentiment_type: Positive\n",
            "Readability_type: Moderate Readability\n",
            "Politeness type: Moderately Polite\n",
            "Coherency_type: Good\n",
            "Email_structure_score: 3/3\n",
            "Email_structure_type: {'has_subject': True, 'has_greeting': True, 'has_closing': True}\n",
            "Overall Score: 9.206349206349206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CfAzXKukT02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}